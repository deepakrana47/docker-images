# Base Python 3.10 image
FROM alpine:3.10

# Expose Port
EXPOSE 8888 4040

# Change shell to /bin/bash
# SHELL ["/bin/bash", "-c"]

# Install OpenJDK, PIP
RUN apk add --no-cache curl bash openjdk8-jre python3 py-pip nss libc6-compat coreutils procps
RUN ln -s /lib64/ld-linux-x86-64.so.2 /lib/ld-linux-x86-64.so.2

# # Install OpenJDK
# RUN apt-get update && \
#     apt install -y openjdk8-jre && \
#     apt-get clean;
#
# # Fix certificate issues
# RUN apt-get install ca-certificates-java && \
#     apt-get clean && \
#     update-ca-certificates -f;
#
# # Insatall nano & vi
# RUN apt-get install -y nano && \
#     apt-get install -y vim;


# Upgrade pip
RUN pip install --upgrade pip

# Setup JAVA_HOME -- useful for docker commandline
ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/
RUN export JAVA_HOME

# Download and Setup Spark binaries
WORKDIR /tmp
RUN wget https://archive.apache.org/dist/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz
RUN tar -xvf spark-3.5.5-bin-hadoop3.tgz
RUN mv spark-3.5.5-bin-hadoop3 spark
RUN mv spark /
RUN rm spark-3.5.5-bin-hadoop3.tgz

# Set up environment variables
ENV SPARK_HOME /spark
RUN export SPARK_HOME
ENV PYSPARK_PYTHON /usr/local/bin/python
RUN export PYSPARK_PYTHON
ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip
RUN export PYTHONPATH
ENV PATH $PATH:$SPARK_HOME/bin
RUN export PATH

# Fix configuration files
RUN mv $SPARK_HOME/conf/log4j2.properties.template $SPARK_HOME/conf/log4j2.properties
RUN mv $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf
RUN mv $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh

# Install Jupyter Lab, PySpark, Kafka, boto & Delta Lake
RUN apk add python3-dev
RUN pip3 install --upgrade pip setuptools wheel
RUN pip3 install --upgrade --force-reinstall --no-cache-dir jupyter
RUN pip3 install pyspark==3.5.5
RUN pip3 install kafka-python==2.0.2
RUN pip3 install delta-spark==2.2.0
RUN pip3 install boto3

# Change to working directory and clone git repo
WORKDIR /home/jupyter

# Clone Ease with Apache Spark Repo to Start
RUN git clone https://github.com/subhamkharwal/ease-with-apache-spark.git

# Fix Jupyter logging issue
RUN ipython profile create
RUN echo "c.IPKernelApp.capture_fd_output = False" >> "/root/.ipython/profile_default/ipython_kernel_config.py"

# Start the container with root privilages
CMD ["python3", "-m", "jupyterlab", "--ip", "0.0.0.0", "--allow-root"]
